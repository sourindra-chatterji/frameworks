#SPARK
    ##Resource Material:
        1. https://data-flair.training/blogs/spark-tutorial/

    ##Apache Spark Components
        https://data-flair.training/blogs/apache-spark-ecosystem-components/
        1. Spark Core
        2. Spark SQL
            https://data-flair.training/blogs/spark-sql-tutorial/
        3. Spark Streaming
            https://data-flair.training/blogs/apache-spark-streaming-tutorial/
        4. Spark MLlib
        5. Spark GraphX
        6. SparkR
            https://data-flair.training/blogs/sparkr/

    ##Resilient Distributed Dataset â€“ RDD
        https://data-flair.training/blogs/apache-spark-rdd-tutorial/

        ###Ways to create Spark RDD
        https://data-flair.training/blogs/create-rdds-in-apache-spark/
        1. Parallelized collections
        2. External datasets
        3. Existing RDDs

        ###Spark RDDs operations
        https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/
        Transformation Operations
        Action Operations

        ###Sparkling Features of Spark RDD
        https://data-flair.training/blogs/apache-spark-rdd-features/
        01. In-memory computation
        02. Lazy Evaluation
            https://data-flair.training/blogs/apache-spark-lazy-evaluation/
        03. Fault Tolerance: write ahead logs
            https://data-flair.training/blogs/fault-tolerance-in-apache-spark/
        04. Immutability
        05. Persistence
            https://data-flair.training/blogs/apache-spark-rdd-persistence-caching/
            http://data-flair.training/blogs/apache-spark-in-memory-computing/
        06. Partitioning
        07. Parallel
        08. Location-Stickiness
            ->  task is close to data as much as possible. Moreover, it speeds up computation.
        09. Coarse-grained Operation
        10. Typed
        11. No limitation on # of rdd creation

    ##Features of Apache Spark
        https://data-flair.training/blogs/apache-spark-features/
        01. Swift Processing
        02. Dynamic in Nature
        03. In-Memory Computation in Spark
        04. Reusability
        05. Spark Fault Tolerance
        06. Real-Time Stream Processing
        07. Lazy Evaluation in Spark
        08. Support Multiple Languages
        09. Support for Sophisticated Analysis
        10. Integrated with Hadoop
        11. Spark GraphX
        12. Cost Efficient: replication

    ##Limitations of Apache Spark Programming
        https://data-flair.training/blogs/limitations-of-apache-spark/
        01. No Support for Real-time Processing
        02. Problem with Small File
        03. No File Management System
        04. Expensive: keeping data in memory is quite expensive
        05. Less number of Algorithms
        06. Manual Optimization
        07. Iterative Processing
        08. Latency
        09. Window Criteria
